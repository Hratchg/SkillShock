# SkillShock Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Build a 4-module Python pipeline that parses People Data JSONL.GZ files into SQLite, computes 5 career trajectory metrics, exports a structured JSON payload, and pushes it to RapidFire AI.

**Architecture:** `ingest.py` → `analytics.py` → `export.py` → `push.py`, orchestrated by `main.py`. Each module is independently runnable. SQLite is the intermediate store. Output is a single `output.json` uploaded to RapidFire AI.

**Tech Stack:** Python 3.10+, SQLite (stdlib), pandas, requests, python-dotenv, pytest

---

## Task 1: Project Scaffold

**Files:**
- Create: `requirements.txt`
- Create: `.env.example`
- Create: `README.md`
- Create: `tests/__init__.py`

**Step 1: Create `requirements.txt`**

```
pandas>=2.0
requests>=2.31
python-dotenv>=1.0
pytest>=8.0
```

**Step 2: Create `.env.example`**

```
DATA_DIR=/mnt/data
DB_PATH=./skillshock.db
OUTPUT_PATH=./output.json
RAPIDFIRE_API_KEY=
RAPIDFIRE_UPLOAD_URL=
```

**Step 3: Create `README.md`**

```markdown
# SkillShock

Career outcome intelligence pipeline. Parses People Data → computes analytics → pushes to RapidFire AI dashboard.

## Setup

```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
# Edit .env with your DATA_DIR and RAPIDFIRE_API_KEY
```

## Run

```bash
python main.py
```

## Test

```bash
pytest tests/ -v
```

## Modules

| File | Role |
|---|---|
| `ingest.py` | Parse JSONL.GZ → SQLite |
| `analytics.py` | Compute 5 metrics from SQLite |
| `export.py` | Shape metrics into output.json |
| `push.py` | POST output.json to RapidFire AI |
| `main.py` | Orchestrate all 4 steps |
```

**Step 4: Create `tests/__init__.py`** (empty file)

**Step 5: Install dependencies**

```bash
pip install -r requirements.txt
```

**Step 6: Commit**

```bash
git add requirements.txt .env.example README.md tests/__init__.py
git commit -m "chore: project scaffold"
```

---

## Task 2: Inspect Data Schema

Before writing ingest code you MUST understand the actual JSONL structure. The schema is unknown — do not skip this step.

**Files:**
- No files created — this is a read-only inspection

**Step 1: Print first record**

```bash
python -c "
import gzip, json, os
from dotenv import load_dotenv
load_dotenv()
data_dir = os.getenv('DATA_DIR', '/mnt/data')
f = sorted(__import__('pathlib').Path(data_dir).glob('live_data_persons_history_*.jsonl.gz'))[0]
with gzip.open(f, 'rt') as fh:
    print(json.dumps(json.loads(fh.readline()), indent=2))
"
```

**Step 2: Document what you see**

Look for:
- Top-level keys (is `jobs` a list? is `location` nested or flat?)
- Job record field names (is it `level` or `seniority`? `company_name` or `company`?)
- Education field names (is it `field` or `major`?)
- Changes field names (flat or nested under `changes`?)
- Date format (ISO 8601? `YYYY-MM-DD`? timestamp?)

**Step 3: Update field name mappings in Task 4 ingest.py**

If field names differ from the design doc, note the actual names here before proceeding:

```
# Actual schema notes (fill in after inspection):
# jobs list key: ___
# job level key: ___
# job company key: ___
# location structure: ___
# date format: ___
```

No commit — this is discovery only.

---

## Task 3: Test Fixture

Create a minimal JSONL.GZ fixture (20 records) that covers all analytics scenarios.

**Files:**
- Create: `tests/make_fixture.py`
- Create: `tests/sample.jsonl.gz` (generated by the script)

**Step 1: Create `tests/make_fixture.py`**

```python
"""Generate tests/sample.jsonl.gz — 20 synthetic People Data records."""
import gzip
import json
import random
from pathlib import Path

random.seed(42)

MAJORS = ["Computer Science", "Business", "Mechanical Engineering", "Biology", "Finance"]
INDUSTRIES = ["Technology", "Finance", "Healthcare", "Consulting", "Education"]
PATHS = [
    ["Software Engineer", "Senior Software Engineer", "Staff Engineer", "Director of Engineering"],
    ["Data Analyst", "Senior Data Analyst", "Data Science Manager", "VP of Data"],
    ["Business Analyst", "Product Manager", "Senior PM", "Director of Product"],
    ["Consultant", "Senior Consultant", "Manager", "VP"],
]
LEVELS = {
    "Software Engineer": "IC",
    "Senior Software Engineer": "Senior",
    "Staff Engineer": "Staff",
    "Director of Engineering": "Director",
    "Data Analyst": "IC",
    "Senior Data Analyst": "Senior",
    "Data Science Manager": "Manager",
    "VP of Data": "VP",
    "Business Analyst": "IC",
    "Product Manager": "IC",
    "Senior PM": "Senior",
    "Director of Product": "Director",
    "Consultant": "IC",
    "Senior Consultant": "Senior",
    "Manager": "Manager",
    "VP": "VP",
}

records = []
for i in range(20):
    path = random.choice(PATHS)
    major = random.choice(MAJORS)
    industry = random.choice(INDUSTRIES)
    start_year = 2010 + random.randint(0, 5)
    jobs = []
    year = start_year + 4  # graduate ~4 years after school starts
    for j, title in enumerate(path):
        duration = random.randint(18, 42)
        started = f"{year}-{random.randint(1,12):02d}-01"
        end_year = year + duration // 12
        end_month = (year_month := (random.randint(1,12))  )
        ended = f"{end_year}-{random.randint(1,12):02d}-01" if j < len(path)-1 else None
        jobs.append({
            "title": title,
            "function": "Engineering" if "Engineer" in title else "Business",
            "level": LEVELS.get(title, "IC"),
            "company_name": f"Company{random.randint(1,5)}",
            "company_industry": industry if j == 0 else random.choice(INDUSTRIES),
            "started_at": started,
            "ended_at": ended,
            "duration": duration,
            "company_tenure": duration,
        })
        year = end_year + 1

    record = {
        "id": f"person_{i:03d}",
        "created_at": "2024-01-01",
        "employment_status": "employed",
        "connections": random.randint(100, 800),
        "location": {"country": "US", "city": random.choice(["San Francisco", "New York", "Austin"])},
        "jobs": jobs,
        "education": [{
            "school": f"University{random.randint(1,5)}",
            "degree": "BS",
            "field": major,
            "started_at": f"{start_year}-09-01",
            "ended_at": f"{start_year+4}-05-01",
        }],
        "changes": {
            "title_change_detected_at": "2023-06-01",
            "company_change_detected_at": None,
            "info_change_detected_at": "2024-01-01",
        },
    }
    records.append(record)

out = Path(__file__).parent / "sample.jsonl.gz"
with gzip.open(out, "wt", encoding="utf-8") as f:
    for r in records:
        f.write(json.dumps(r) + "\n")

print(f"Written {len(records)} records to {out}")
```

**Step 2: Generate the fixture**

```bash
python tests/make_fixture.py
```

Expected output: `Written 20 records to tests/sample.jsonl.gz`

**Step 3: Verify it's readable**

```bash
python -c "
import gzip, json
from pathlib import Path
with gzip.open('tests/sample.jsonl.gz', 'rt') as f:
    records = [json.loads(l) for l in f]
print(f'{len(records)} records, first id: {records[0][\"id\"]}')
"
```

Expected: `20 records, first id: person_000`

**Step 4: Commit**

```bash
git add tests/make_fixture.py tests/sample.jsonl.gz
git commit -m "test: add sample fixture generator and 20-record fixture"
```

---

## Task 4: `ingest.py` (TDD)

Parse JSONL.GZ files into SQLite `persons`, `jobs`, `education`, `changes` tables.

**Files:**
- Create: `tests/test_ingest.py`
- Create: `ingest.py`

**Step 1: Write the failing tests**

```python
# tests/test_ingest.py
import sqlite3
import tempfile
from pathlib import Path
import pytest
import ingest

FIXTURE = Path(__file__).parent / "sample.jsonl.gz"

@pytest.fixture
def db():
    with tempfile.NamedTemporaryFile(suffix=".db") as f:
        conn = sqlite3.connect(f.name)
        ingest.create_tables(conn)
        yield conn
        conn.close()

def test_create_tables_makes_all_four(db):
    tables = {r[0] for r in db.execute("SELECT name FROM sqlite_master WHERE type='table'").fetchall()}
    assert tables == {"persons", "jobs", "education", "changes"}

def test_ingest_file_loads_records(db):
    loaded, skipped = ingest.ingest_file(FIXTURE, db)
    assert loaded == 20
    assert skipped == 0

def test_persons_table_populated(db):
    ingest.ingest_file(FIXTURE, db)
    count = db.execute("SELECT COUNT(*) FROM persons").fetchone()[0]
    assert count == 20

def test_jobs_table_populated(db):
    ingest.ingest_file(FIXTURE, db)
    count = db.execute("SELECT COUNT(*) FROM jobs").fetchone()[0]
    assert count > 20  # each person has multiple jobs

def test_education_table_populated(db):
    ingest.ingest_file(FIXTURE, db)
    count = db.execute("SELECT COUNT(*) FROM education").fetchone()[0]
    assert count == 20

def test_duration_months_computed(db):
    ingest.ingest_file(FIXTURE, db)
    nulls = db.execute("SELECT COUNT(*) FROM jobs WHERE started_at IS NOT NULL AND ended_at IS NOT NULL AND duration_months IS NULL").fetchone()[0]
    assert nulls == 0

def test_level_normalized(db):
    ingest.ingest_file(FIXTURE, db)
    valid_levels = {"IC", "Senior", "Staff", "Manager", "Director", "VP", "C-Suite", "Unknown"}
    levels = {r[0] for r in db.execute("SELECT DISTINCT level FROM jobs").fetchall()}
    assert levels.issubset(valid_levels)

def test_run_accepts_data_dir(tmp_path, db):
    import shutil
    shutil.copy(FIXTURE, tmp_path / "live_data_persons_history_2026-02-19_00.jsonl.gz")
    db_path = str(tmp_path / "test.db")
    total = ingest.run(str(tmp_path), db_path)
    assert total == 20

def test_malformed_lines_skipped(db, tmp_path):
    import gzip
    bad_file = tmp_path / "live_data_persons_history_bad.jsonl.gz"
    with gzip.open(bad_file, "wt") as f:
        f.write('{"id": "ok", "jobs": [], "education": [], "changes": {}}\n')
        f.write('NOT JSON\n')
        f.write('{"id": "ok2", "jobs": [], "education": [], "changes": {}}\n')
    loaded, skipped = ingest.ingest_file(bad_file, db)
    assert loaded == 2
    assert skipped == 1
```

**Step 2: Run tests to verify they fail**

```bash
pytest tests/test_ingest.py -v
```

Expected: All FAIL with `ModuleNotFoundError: No module named 'ingest'`

**Step 3: Implement `ingest.py`**

```python
"""ingest.py — Parse JSONL.GZ files into SQLite tables."""
import gzip
import json
import logging
import os
import sqlite3
from datetime import datetime
from pathlib import Path

logging.basicConfig(level=logging.INFO, format="%(levelname)s %(message)s")
logger = logging.getLogger(__name__)

# Map raw level/seniority strings → normalized vocabulary
LEVEL_KEYWORDS = [
    ("c-suite", "C-Suite"), ("chief", "C-Suite"), ("ceo", "C-Suite"),
    ("cto", "C-Suite"), ("cfo", "C-Suite"), ("coo", "C-Suite"),
    ("evp", "VP"), ("svp", "VP"), ("vice president", "VP"), ("vp", "VP"),
    ("senior director", "Director"), ("sr. director", "Director"), ("director", "Director"),
    ("senior manager", "Manager"), ("sr. manager", "Manager"), ("manager", "Manager"),
    ("principal", "Staff"), ("staff", "Staff"), ("lead", "Staff"),
    ("senior", "Senior"), ("sr.", "Senior"), ("sr ", "Senior"),
    ("junior", "IC"), ("associate", "IC"), ("entry", "IC"),
]

def normalize_level(raw):
    if not raw:
        return "Unknown"
    lower = raw.lower()
    for keyword, normalized in LEVEL_KEYWORDS:
        if keyword in lower:
            return normalized
    return "Unknown"

def months_between(start_str, end_str):
    if not start_str or not end_str:
        return None
    try:
        start = datetime.fromisoformat(str(start_str)[:10])
        end = datetime.fromisoformat(str(end_str)[:10])
        return max(0, (end.year - start.year) * 12 + (end.month - start.month))
    except (ValueError, TypeError):
        return None

def create_tables(conn):
    conn.executescript("""
        CREATE TABLE IF NOT EXISTS persons (
            id TEXT PRIMARY KEY,
            created_at TEXT,
            employment_status TEXT,
            connections INTEGER,
            location_country TEXT,
            location_city TEXT
        );
        CREATE TABLE IF NOT EXISTS jobs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            person_id TEXT,
            title TEXT,
            function TEXT,
            level TEXT,
            company_name TEXT,
            company_industry TEXT,
            started_at TEXT,
            ended_at TEXT,
            duration_months INTEGER,
            company_tenure_months INTEGER
        );
        CREATE TABLE IF NOT EXISTS education (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            person_id TEXT,
            school TEXT,
            degree TEXT,
            field TEXT,
            started_at TEXT,
            ended_at TEXT
        );
        CREATE TABLE IF NOT EXISTS changes (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            person_id TEXT,
            title_change_detected_at TEXT,
            company_change_detected_at TEXT,
            info_change_detected_at TEXT
        );
    """)
    conn.commit()

def load_record(record, conn):
    person_id = record.get("id")
    if not person_id:
        raise KeyError("missing id")

    loc = record.get("location") or {}
    if not isinstance(loc, dict):
        loc = {}

    conn.execute(
        "INSERT OR IGNORE INTO persons VALUES (?,?,?,?,?,?)",
        (
            person_id,
            record.get("created_at"),
            record.get("employment_status"),
            record.get("connections"),
            loc.get("country") or record.get("location_country"),
            loc.get("city") or record.get("location_city"),
        ),
    )

    for job in record.get("jobs") or []:
        started = job.get("started_at")
        ended = job.get("ended_at")
        raw_level = job.get("level") or job.get("seniority") or ""
        conn.execute(
            """INSERT INTO jobs
               (person_id,title,function,level,company_name,company_industry,
                started_at,ended_at,duration_months,company_tenure_months)
               VALUES (?,?,?,?,?,?,?,?,?,?)""",
            (
                person_id,
                job.get("title"),
                job.get("function"),
                normalize_level(raw_level),
                job.get("company_name") or job.get("company"),
                job.get("company_industry") or job.get("industry"),
                started,
                ended,
                months_between(started, ended),
                job.get("company_tenure") or months_between(started, ended),
            ),
        )

    for edu in record.get("education") or []:
        conn.execute(
            """INSERT INTO education (person_id,school,degree,field,started_at,ended_at)
               VALUES (?,?,?,?,?,?)""",
            (
                person_id,
                edu.get("school"),
                edu.get("degree"),
                edu.get("field") or edu.get("major"),
                edu.get("started_at"),
                edu.get("ended_at"),
            ),
        )

    chg = record.get("changes") or {}
    if isinstance(chg, dict):
        conn.execute(
            """INSERT INTO changes
               (person_id,title_change_detected_at,company_change_detected_at,info_change_detected_at)
               VALUES (?,?,?,?)""",
            (
                person_id,
                chg.get("title_change_detected_at"),
                chg.get("company_change_detected_at"),
                chg.get("info_change_detected_at"),
            ),
        )

def ingest_file(filepath, conn):
    loaded = skipped = 0
    with gzip.open(filepath, "rt", encoding="utf-8") as f:
        for lineno, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                record = json.loads(line)
                load_record(record, conn)
                loaded += 1
            except (json.JSONDecodeError, KeyError) as e:
                logger.warning(f"{filepath}:{lineno} skipped — {e}")
                skipped += 1
    conn.commit()
    logger.info(f"{Path(filepath).name}: loaded={loaded} skipped={skipped}")
    return loaded, skipped

def run(data_dir, db_path):
    files = sorted(Path(data_dir).glob("live_data_persons_history_*.jsonl.gz"))
    if not files:
        raise FileNotFoundError(f"No JSONL.GZ files in {data_dir}")
    conn = sqlite3.connect(db_path)
    create_tables(conn)
    total = sum(ingest_file(f, conn)[0] for f in files)
    conn.close()
    logger.info(f"Ingest complete — {total} total records, db: {db_path}")
    return total

if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv()
    run(
        data_dir=os.getenv("DATA_DIR", "/mnt/data"),
        db_path=os.getenv("DB_PATH", "./skillshock.db"),
    )
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_ingest.py -v
```

Expected: All PASS

**Step 5: Commit**

```bash
git add ingest.py tests/test_ingest.py
git commit -m "feat: implement ingest — parse JSONL.GZ into SQLite"
```

---

## Task 5: `analytics.py` (TDD)

Compute 5 aggregated metrics from SQLite. Uses pandas for aggregations.

**Files:**
- Create: `tests/test_analytics.py`
- Create: `analytics.py`

**Step 1: Write the failing tests**

```python
# tests/test_analytics.py
import sqlite3
import tempfile
from pathlib import Path
import pytest
import ingest
import analytics

FIXTURE = Path(__file__).parent / "sample.jsonl.gz"

@pytest.fixture
def populated_db():
    with tempfile.NamedTemporaryFile(suffix=".db", delete=False) as f:
        db_path = f.name
    conn = sqlite3.connect(db_path)
    ingest.create_tables(conn)
    ingest.ingest_file(FIXTURE, conn)
    conn.close()
    yield db_path

def test_promotion_velocity_returns_dict(populated_db):
    result = analytics.promotion_velocity(populated_db)
    assert isinstance(result, dict)

def test_promotion_velocity_keys_have_required_fields(populated_db):
    result = analytics.promotion_velocity(populated_db)
    for key, val in result.items():
        assert "median_months" in val, f"{key} missing median_months"
        assert "sample_size" in val, f"{key} missing sample_size"
        assert "low_confidence" in val, f"{key} missing low_confidence"

def test_role_transitions_returns_dict(populated_db):
    result = analytics.role_transitions(populated_db)
    assert isinstance(result, dict)

def test_role_transitions_probabilities_sum_to_one(populated_db):
    result = analytics.role_transitions(populated_db)
    for from_role, to_roles in result.items():
        total = sum(v for v in to_roles.values() if isinstance(v, float))
        assert abs(total - 1.0) < 0.01, f"{from_role} probs sum to {total}"

def test_major_to_first_role_returns_dict(populated_db):
    result = analytics.major_to_first_role(populated_db)
    assert isinstance(result, dict)
    assert len(result) > 0

def test_major_to_first_role_top_10_max(populated_db):
    result = analytics.major_to_first_role(populated_db)
    for major, roles in result.items():
        assert len(roles) <= 10, f"{major} has more than 10 roles"

def test_industry_transitions_returns_dict(populated_db):
    result = analytics.industry_transitions(populated_db)
    assert isinstance(result, dict)

def test_paths_to_role_returns_dict(populated_db):
    result = analytics.paths_to_role(populated_db)
    assert isinstance(result, dict)

def test_paths_to_role_entries_have_path_and_frequency(populated_db):
    result = analytics.paths_to_role(populated_db)
    for role, paths in result.items():
        for entry in paths:
            assert "path" in entry
            assert "frequency" in entry
            assert isinstance(entry["path"], list)

def test_compute_all_returns_all_five_keys(populated_db):
    result = analytics.compute_all(populated_db)
    assert set(result.keys()) == {
        "promotion_velocity",
        "role_transitions",
        "major_to_first_role",
        "industry_transitions",
        "paths_to_role",
    }
```

**Step 2: Run tests to verify they fail**

```bash
pytest tests/test_analytics.py -v
```

Expected: All FAIL with `ModuleNotFoundError: No module named 'analytics'`

**Step 3: Implement `analytics.py`**

```python
"""analytics.py — Compute 5 career trajectory metrics from SQLite."""
import logging
import os
import sqlite3

import pandas as pd

logger = logging.getLogger(__name__)

LEVEL_ORDER = ["IC", "Senior", "Staff", "Manager", "Director", "VP", "C-Suite"]


def _jobs_df(db_path):
    conn = sqlite3.connect(db_path)
    df = pd.read_sql("SELECT * FROM jobs", conn)
    conn.close()
    df["started_at"] = pd.to_datetime(df["started_at"], errors="coerce")
    df["ended_at"] = pd.to_datetime(df["ended_at"], errors="coerce")
    return df


def _edu_df(db_path):
    conn = sqlite3.connect(db_path)
    df = pd.read_sql("SELECT * FROM education", conn)
    conn.close()
    return df


def promotion_velocity(db_path):
    """Median months between consecutive level transitions, per from→to level pair."""
    jobs = _jobs_df(db_path)
    jobs = jobs[jobs["level"].isin(LEVEL_ORDER)].sort_values(["person_id", "started_at"])
    jobs["next_level"] = jobs.groupby("person_id")["level"].shift(-1)
    jobs["next_started"] = jobs.groupby("person_id")["started_at"].shift(-1)

    transitions = jobs[
        jobs["level"].ne(jobs["next_level"])
        & jobs["next_level"].notna()
        & jobs["started_at"].notna()
        & jobs["next_started"].notna()
    ].copy()
    transitions["gap_months"] = (
        (transitions["next_started"] - transitions["started_at"]).dt.days / 30.44
    ).round()

    result = {}
    for (from_lvl, to_lvl), group in transitions.groupby(["level", "next_level"]):
        key = f"{from_lvl}_to_{to_lvl}"
        n = len(group)
        result[key] = {
            "median_months": int(group["gap_months"].median()),
            "sample_size": n,
            "low_confidence": n < 10,
        }
    return result


def role_transitions(db_path):
    """For each title, probability distribution of the next title."""
    jobs = _jobs_df(db_path)
    jobs = jobs.sort_values(["person_id", "started_at"])
    jobs["next_title"] = jobs.groupby("person_id")["title"].shift(-1)
    transitions = jobs[jobs["next_title"].notna() & jobs["title"].notna()]

    result = {}
    for from_title, group in transitions.groupby("title"):
        counts = group["next_title"].value_counts()
        total = counts.sum()
        result[from_title] = {title: round(cnt / total, 4) for title, cnt in counts.items()}
    return result


def major_to_first_role(db_path):
    """For each education field, top 10 first job titles by frequency."""
    edu = _edu_df(db_path)
    jobs = _jobs_df(db_path).sort_values(["person_id", "started_at"])
    first_jobs = jobs.groupby("person_id").first().reset_index()[["person_id", "title"]]

    merged = edu.merge(first_jobs, on="person_id")
    merged = merged[merged["field"].notna() & merged["title"].notna()]

    result = {}
    for field, group in merged.groupby("field"):
        counts = group["title"].value_counts().head(10)
        total = counts.sum()
        result[field] = {title: round(cnt / total, 4) for title, cnt in counts.items()}
    return result


def industry_transitions(db_path):
    """For each industry, probability distribution of next industry."""
    jobs = _jobs_df(db_path).sort_values(["person_id", "started_at"])
    jobs["next_industry"] = jobs.groupby("person_id")["company_industry"].shift(-1)
    transitions = jobs[
        jobs["next_industry"].notna()
        & jobs["company_industry"].notna()
        & jobs["company_industry"].ne(jobs["next_industry"])
    ]

    result = {}
    for from_ind, group in transitions.groupby("company_industry"):
        counts = group["next_industry"].value_counts()
        total = counts.sum()
        result[from_ind] = {ind: round(cnt / total, 4) for ind, cnt in counts.items()}
    return result


def paths_to_role(db_path):
    """Top 5 ordered title sequences ending at each final title, with frequency."""
    jobs = _jobs_df(db_path).sort_values(["person_id", "started_at"])
    jobs = jobs[jobs["title"].notna()]

    path_counts: dict[str, dict[tuple, int]] = {}
    for person_id, group in jobs.groupby("person_id"):
        titles = tuple(group["title"].tolist())
        if len(titles) < 2:
            continue
        final = titles[-1]
        path_counts.setdefault(final, {})
        path_counts[final][titles] = path_counts[final].get(titles, 0) + 1

    result = {}
    for final_role, paths in path_counts.items():
        sorted_paths = sorted(paths.items(), key=lambda x: -x[1])[:5]
        result[final_role] = [
            {"path": list(path), "frequency": freq} for path, freq in sorted_paths
        ]
    return result


def compute_all(db_path):
    """Run all 5 metrics and return combined dict."""
    logger.info("Computing promotion_velocity...")
    pv = promotion_velocity(db_path)
    logger.info("Computing role_transitions...")
    rt = role_transitions(db_path)
    logger.info("Computing major_to_first_role...")
    mf = major_to_first_role(db_path)
    logger.info("Computing industry_transitions...")
    it = industry_transitions(db_path)
    logger.info("Computing paths_to_role...")
    pr = paths_to_role(db_path)
    logger.info("Analytics complete.")
    return {
        "promotion_velocity": pv,
        "role_transitions": rt,
        "major_to_first_role": mf,
        "industry_transitions": it,
        "paths_to_role": pr,
    }


if __name__ == "__main__":
    import json
    from dotenv import load_dotenv
    load_dotenv()
    db_path = os.getenv("DB_PATH", "./skillshock.db")
    result = compute_all(db_path)
    print(json.dumps(result, indent=2, default=str))
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_analytics.py -v
```

Expected: All PASS

**Step 5: Commit**

```bash
git add analytics.py tests/test_analytics.py
git commit -m "feat: implement analytics — 5 career trajectory metrics"
```

---

## Task 6: `export.py` (TDD)

Shape analytics dicts + metadata into the `output.json` schema.

**Files:**
- Create: `tests/test_export.py`
- Create: `export.py`

**Step 1: Write the failing tests**

```python
# tests/test_export.py
import json
import sqlite3
import tempfile
from pathlib import Path
import pytest
import ingest
import analytics
import export

FIXTURE = Path(__file__).parent / "sample.jsonl.gz"

@pytest.fixture
def metrics_and_db():
    with tempfile.NamedTemporaryFile(suffix=".db", delete=False) as f:
        db_path = f.name
    conn = sqlite3.connect(db_path)
    ingest.create_tables(conn)
    ingest.ingest_file(FIXTURE, conn)
    conn.close()
    metrics = analytics.compute_all(db_path)
    return metrics, db_path

def test_build_payload_has_required_top_level_keys(metrics_and_db):
    metrics, db_path = metrics_and_db
    payload = export.build_payload(metrics, db_path, data_files=["file00.jsonl.gz"])
    required = {"metadata", "promotion_velocity", "role_transitions",
                "major_to_first_role", "industry_transitions", "paths_to_role"}
    assert required.issubset(set(payload.keys()))

def test_metadata_has_required_fields(metrics_and_db):
    metrics, db_path = metrics_and_db
    payload = export.build_payload(metrics, db_path, data_files=["file00.jsonl.gz"])
    meta = payload["metadata"]
    assert "generated_at" in meta
    assert "total_persons" in meta
    assert "total_jobs" in meta
    assert "data_files" in meta
    assert isinstance(meta["data_files"], list)

def test_metadata_counts_match_db(metrics_and_db):
    metrics, db_path = metrics_and_db
    payload = export.build_payload(metrics, db_path, data_files=[])
    assert payload["metadata"]["total_persons"] == 20

def test_payload_is_json_serializable(metrics_and_db):
    metrics, db_path = metrics_and_db
    payload = export.build_payload(metrics, db_path, data_files=[])
    # Should not raise
    serialized = json.dumps(payload)
    assert len(serialized) > 100

def test_save_writes_file(metrics_and_db, tmp_path):
    metrics, db_path = metrics_and_db
    out = tmp_path / "output.json"
    payload = export.build_payload(metrics, db_path, data_files=[])
    export.save(payload, str(out))
    assert out.exists()
    loaded = json.loads(out.read_text())
    assert "metadata" in loaded
```

**Step 2: Run tests to verify they fail**

```bash
pytest tests/test_export.py -v
```

Expected: All FAIL with `ModuleNotFoundError: No module named 'export'`

**Step 3: Implement `export.py`**

```python
"""export.py — Shape analytics results into output.json for RapidFire AI."""
import json
import logging
import os
import sqlite3
from datetime import datetime, timezone
from pathlib import Path

logger = logging.getLogger(__name__)


def _db_counts(db_path):
    conn = sqlite3.connect(db_path)
    persons = conn.execute("SELECT COUNT(*) FROM persons").fetchone()[0]
    jobs = conn.execute("SELECT COUNT(*) FROM jobs").fetchone()[0]
    conn.close()
    return persons, jobs


def build_payload(metrics, db_path, data_files):
    total_persons, total_jobs = _db_counts(db_path)
    payload = {
        "metadata": {
            "generated_at": datetime.now(timezone.utc).isoformat(),
            "total_persons": total_persons,
            "total_jobs": total_jobs,
            "data_files": [str(Path(f).name) for f in data_files],
        },
        **metrics,
    }
    return payload


def save(payload, output_path):
    Path(output_path).write_text(
        json.dumps(payload, indent=2, default=str), encoding="utf-8"
    )
    logger.info(f"Saved output to {output_path}")


def run(metrics, db_path, data_files, output_path):
    payload = build_payload(metrics, db_path, data_files)
    save(payload, output_path)
    return payload


if __name__ == "__main__":
    import analytics
    from dotenv import load_dotenv
    load_dotenv()
    db_path = os.getenv("DB_PATH", "./skillshock.db")
    output_path = os.getenv("OUTPUT_PATH", "./output.json")
    metrics = analytics.compute_all(db_path)
    run(metrics, db_path, data_files=[], output_path=output_path)
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_export.py -v
```

Expected: All PASS

**Step 5: Commit**

```bash
git add export.py tests/test_export.py
git commit -m "feat: implement export — shape analytics into output.json"
```

---

## Task 7: `push.py`

POST `output.json` to RapidFire AI. Gracefully skip if API key missing.

**Files:**
- Create: `push.py`

> No automated test — this is a live API call. Verified manually during demo.

**Step 1: Implement `push.py`**

```python
"""push.py — Upload output.json to RapidFire AI."""
import json
import logging
import os
from pathlib import Path

import requests

logger = logging.getLogger(__name__)


def run(output_path, api_key, upload_url):
    if not api_key:
        logger.warning("RAPIDFIRE_API_KEY not set — skipping push. Output saved locally.")
        return False

    if not upload_url:
        logger.warning("RAPIDFIRE_UPLOAD_URL not set — skipping push.")
        return False

    payload = json.loads(Path(output_path).read_text(encoding="utf-8"))

    logger.info(f"Pushing to RapidFire AI: {upload_url}")
    response = requests.post(
        upload_url,
        json=payload,
        headers={
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        },
        timeout=60,
    )
    response.raise_for_status()
    logger.info(f"Push successful — status {response.status_code}")
    return True


if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv()
    run(
        output_path=os.getenv("OUTPUT_PATH", "./output.json"),
        api_key=os.getenv("RAPIDFIRE_API_KEY", ""),
        upload_url=os.getenv("RAPIDFIRE_UPLOAD_URL", ""),
    )
```

**Step 2: Commit**

```bash
git add push.py
git commit -m "feat: implement push — POST output.json to RapidFire AI"
```

---

## Task 8: `main.py` — Orchestrator

Wire all 4 modules together sequentially.

**Files:**
- Create: `main.py`

**Step 1: Implement `main.py`**

```python
"""main.py — Run the full SkillShock pipeline."""
import logging
import os
import sys
from pathlib import Path

from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(level=logging.INFO, format="%(levelname)s %(message)s")
logger = logging.getLogger(__name__)

import analytics
import export
import ingest
import push


def main():
    data_dir   = os.getenv("DATA_DIR",   "/mnt/data")
    db_path    = os.getenv("DB_PATH",    "./skillshock.db")
    output_path = os.getenv("OUTPUT_PATH", "./output.json")
    api_key    = os.getenv("RAPIDFIRE_API_KEY", "")
    upload_url = os.getenv("RAPIDFIRE_UPLOAD_URL", "")

    data_files = sorted(Path(data_dir).glob("live_data_persons_history_*.jsonl.gz"))

    # Step 1: Ingest
    logger.info("=== Step 1/4: Ingest ===")
    try:
        total = ingest.run(data_dir, db_path)
        logger.info(f"Ingested {total} records.")
    except Exception as e:
        logger.error(f"Ingest failed: {e}")
        sys.exit(1)

    # Step 2: Analytics
    logger.info("=== Step 2/4: Analytics ===")
    try:
        metrics = analytics.compute_all(db_path)
    except Exception as e:
        logger.error(f"Analytics failed: {e}")
        sys.exit(1)

    # Step 3: Export
    logger.info("=== Step 3/4: Export ===")
    try:
        export.run(metrics, db_path, data_files=[str(f) for f in data_files], output_path=output_path)
        logger.info(f"Output written to {output_path}")
    except Exception as e:
        logger.error(f"Export failed: {e}")
        sys.exit(1)

    # Step 4: Push
    logger.info("=== Step 4/4: Push ===")
    try:
        pushed = push.run(output_path, api_key, upload_url)
        if pushed:
            logger.info("Pipeline complete — dashboard live on RapidFire AI.")
        else:
            logger.info(f"Pipeline complete — output saved locally to {output_path}.")
    except Exception as e:
        logger.error(f"Push failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
```

**Step 2: Run full test suite to verify nothing is broken**

```bash
pytest tests/ -v
```

Expected: All PASS

**Step 3: Smoke test with fixture data (optional dry run)**

```bash
DATA_DIR=tests DB_PATH=./test_smoke.db OUTPUT_PATH=./test_output.json python main.py
```

Note: `tests/` only has `sample.jsonl.gz` — rename it temporarily or adjust glob if needed.

Expected: Pipeline runs 4 steps, exits cleanly, `test_output.json` created.

**Step 4: Commit**

```bash
git add main.py
git commit -m "feat: implement main — orchestrate full pipeline"
```

---

## Task 9: Adapt to Real Data Schema

After running Task 2 (schema inspection), you may need to update `ingest.py` field mappings.

**Files:**
- Modify: `ingest.py` (field name aliases in `load_record`)

**Step 1: Run ingest against real data**

```bash
python ingest.py
```

**Step 2: If KeyError or all-null columns appear, update field aliases in `load_record`**

Common fixes:
- `job.get("company_name") or job.get("company") or job.get("org_name")`
- `edu.get("field") or edu.get("major") or edu.get("study_field")`
- `job.get("level") or job.get("seniority") or job.get("career_level")`

**Step 3: Re-run tests after any changes**

```bash
pytest tests/test_ingest.py -v
```

**Step 4: Commit any schema fixes**

```bash
git add ingest.py
git commit -m "fix: adapt ingest field mappings to actual data schema"
```

---

## Task 10: Final Demo Run

**Step 1: Set `.env` with real paths and API key**

```
DATA_DIR=/mnt/data
DB_PATH=./skillshock.db
OUTPUT_PATH=./output.json
RAPIDFIRE_API_KEY=<your key>
RAPIDFIRE_UPLOAD_URL=<rapidfire upload endpoint>
```

**Step 2: Delete any previous DB**

```bash
rm -f skillshock.db
```

**Step 3: Run full pipeline**

```bash
python main.py
```

Expected output:
```
INFO === Step 1/4: Ingest ===
INFO live_data_persons_history_2026-02-19_00.jsonl.gz: loaded=... skipped=...
INFO live_data_persons_history_2026-02-19_01.jsonl.gz: loaded=... skipped=...
INFO live_data_persons_history_2026-02-19_02.jsonl.gz: loaded=... skipped=...
INFO Ingested N records.
INFO === Step 2/4: Analytics ===
INFO Computing promotion_velocity...
INFO Computing role_transitions...
INFO Computing major_to_first_role...
INFO Computing industry_transitions...
INFO Computing paths_to_role...
INFO Analytics complete.
INFO === Step 3/4: Export ===
INFO Saved output to ./output.json
INFO Output written to ./output.json
INFO === Step 4/4: Push ===
INFO Pushing to RapidFire AI: <url>
INFO Push successful — status 200
INFO Pipeline complete — dashboard live on RapidFire AI.
```

**Step 4: Final commit**

```bash
git add -A
git commit -m "chore: final demo run complete"
```
